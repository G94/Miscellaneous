{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing main Libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VoxivaAI\\AppData\\Local\\conda\\conda\\envs\\cr_env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\VoxivaAI\\AppData\\Local\\conda\\conda\\envs\\cr_env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing main Libraries\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as  sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import re\n",
    "\n",
    "### Configuraciones pandas library\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import matplotlib as mlp\n",
    "mlp.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "### paleta de colores\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Metrics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import sys\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import lightgbm as lgb\n",
    "from urllib.parse import urlparse\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "# import mlflow\n",
    "\n",
    "# import mlflow.sklearn\n",
    "# from mlflow import log_metric, log_param, log_artifact\n",
    "\n",
    "print(\"Importing Metrics\")\n",
    "def getFeatureImportance(model, X_train, canal):\n",
    "    features = pd.DataFrame()\n",
    "    features['feature'] = X_train.columns\n",
    "    features['importance'] = model.feature_importances_\n",
    "    ftrs = features.sort_values(by = ['importance','feature'], ascending=False)\n",
    "    ftrs = ftrs.reset_index(drop = True)\n",
    "    plt.figure(figsize=(12,25))\n",
    "    \n",
    "    ax = sns.barplot(\"importance\", \"feature\", data = ftrs[ftrs.importance>0.009])\n",
    "    \n",
    "    plt.ylabel(\"Variables Relevantes\", fontsize = 24, color = \"white\" )\n",
    "    plt.xlabel(\"Importancia (Frecuencia)\", fontsize = 24, color = \"white\")\n",
    "    plt.show()\n",
    "    \n",
    "    name_fil = 'FeatureImportance_{}.png'.format(canal)\n",
    "    plt.savefig(name_fil, bbox_inches = 'tight', transparent = True) \n",
    "    return ftrs, name_fil\n",
    "\n",
    "\n",
    "\n",
    "def rmsle_score(y_test, y_pred):\n",
    "    result = np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_function(id_meancvs, data_not_duplicates, alpha_uno = 1, alpha_dos = 1, exp = 1):\n",
    "    rows = []\n",
    "    dfs = {}\n",
    "    \n",
    "    id_meancvs['IdMaterial'] = id_meancvs.IdMaterial.astype(int)\n",
    "    data_not_duplicates['IdMaterial'] = data_not_duplicates.IdMaterial.astype(int)\n",
    "    data_not_duplicates = data_not_duplicates[data_not_duplicates.IdMaterial.isin(id_meancvs.IdMaterial.unique())]\n",
    "    \n",
    "    for producto in data_not_duplicates.Descripcion.unique():\n",
    "        data_po = data_not_duplicates[data_not_duplicates['Descripcion'] == producto].sort_values('Huevo_calato_2', \n",
    "                                                                                                    ascending = True)\n",
    "        data_po['log_rentabilidad'] = np.log1p(data_po['rentabilidad'])\n",
    "\n",
    "        data_comparison = pd.merge(data_po, id_meancvs, on = 'IdMaterial')\n",
    "        data_comparison = data_comparison.rename(columns={'CantidadVendida_last_year': 'total_mensual_last_year'})\n",
    "        \n",
    "        ## calculo de diferencial de precio con respecto al año anterior\n",
    "        data_comparison['perprecio_last_year'] = abs((data_comparison.precio_dealer - data_comparison.precio_year_ago)/data_comparison.precio_year_ago)\n",
    "        data_comparison['perprecio_last_month'] = abs((data_comparison.precio_dealer - data_comparison.precio_month_ago)/data_comparison.precio_month_ago)\n",
    "        data_comparison['perprecio_last_month_noabs'] = ((data_comparison.precio_dealer - data_comparison.precio_month_ago)/data_comparison.precio_month_ago)\n",
    "        \n",
    "\n",
    "        data_comparison['percv'] = abs((data_comparison.total_mensual - data_comparison.CantidadVendida_year_ago)/data_comparison.CantidadVendida_year_ago).astype(float)\n",
    "        \n",
    "        # Se reescala el rango del porcentaje de la diferencia usando el exponencial\n",
    "        data_comparison['exp_perprecio'] = np.exp(data_comparison['perprecio_last_month'])\n",
    "        data_comparison['exp_percv'] = np.exp(data_comparison['percv'])\n",
    "        \n",
    "        min_v = data_comparison['exp_perprecio'].min()\n",
    "        max_v = data_comparison['exp_perprecio'].max()\n",
    "        \n",
    "        if len(data_comparison.precio_dealer.unique()) > 1 :\n",
    "            min_max_perprecio = data_comparison['exp_perprecio'].apply(lambda x: minmax(x,min_v,max_v))\n",
    "        else:\n",
    "            min_max_perprecio = 0\n",
    "            \n",
    "        data_comparison['minmax_precio'] = min_max_perprecio\n",
    "        \n",
    "        min_v = data_comparison['exp_percv'].min()\n",
    "        max_v = data_comparison['exp_percv'].max()\n",
    "        \n",
    "        if len(data_comparison.exp_percv.unique()) > 1 :\n",
    "            min_max_percv = data_comparison['exp_percv'].apply(lambda x: minmax(x,min_v,max_v))\n",
    "        else:\n",
    "            min_max_percv = 0\n",
    "            \n",
    "        data_comparison['min_max_percv'] = min_max_percv\n",
    "        A = (1/(1+ alpha_uno*min_max_perprecio))\n",
    "        B = (1/(1+ alpha_dos*min_max_percv))\n",
    "        \n",
    "        data_comparison['A']  = A\n",
    "        data_comparison['B']  = B\n",
    "        \n",
    "        min_v = data_comparison['log_rentabilidad'].min()\n",
    "        max_v = data_comparison['log_rentabilidad'].max()   \n",
    "        \n",
    "        data_comparison['log_minmax'] = data_comparison['log_rentabilidad'].apply(lambda x: minmax(x,min_v,max_v))\n",
    "        \n",
    "        #         data_comparison['score1'] = (exp*data_comparison.log_minmax + A*B)/(exp + 1)\n",
    "        data_comparison['score2'] = (exp*data_comparison.log_minmax + A*B)/(exp + 1)\n",
    "\n",
    "        data_comparison['meanscore'] = (data_comparison['score1'] + data_comparison['score2'])/2.0\n",
    "        \n",
    "        ####\n",
    "        data_row = data_comparison.sort_values('score2', ascending = False).iloc[:1,:]\n",
    "        dfs[id_material] = data_comparison.sort_values('meanscore', ascending = False)\n",
    "        rows.append(data_row)\n",
    "        \n",
    "    pd_results = pd.concat(rows)\n",
    "    return pd_results, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "def report_nulls(df_input):\n",
    "    tab_info = pd.DataFrame(df_input.dtypes).T.rename(index={0:'column Type'}) \n",
    "    tab_info = tab_info.append(pd.DataFrame(df_input.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "    tab_info = tab_info.append(pd.DataFrame(df_input.isnull().sum()/df_input.shape[0]*100).T.\n",
    "                                           rename(index={0: 'null values (%)'}))\n",
    "    return tab_info\n",
    "\n",
    "def get_unique(series):\n",
    "    series  = series.astype(str)\n",
    "    uniques =  np.unique(series)\n",
    "    return len(uniques)\n",
    "\n",
    "def getFirst(series):\n",
    "    val = 0\n",
    "    try:\n",
    "        val = series.values[0]\n",
    "    except:\n",
    "#         print(series)\n",
    "#         print(\"except\")\n",
    "#         sys.exit(0)\n",
    "        val = np.nan\n",
    "    return val\n",
    "\n",
    "def getLast(series):\n",
    "    val = 0\n",
    "    try:\n",
    "        val = series.values[-1] \n",
    "    except:    \n",
    "        val = np.nan\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeekGenerator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    El estimador **WeekGenerator** crea las instancias faltantes en base a una agrupación de fechas.\n",
    "   \n",
    "        Ejemplo::\n",
    "        \n",
    "        from sklearn import datasets\n",
    "        X = datasets.load_iris()   \n",
    "\n",
    "        encoder_aggregator = WeekGenerator(list_features, dict_features)\n",
    "\n",
    "        X_grouped = encoder_aggregator.fit_transform(X)\n",
    "\n",
    "    El ejemplo retorna el dataset con el historico completo.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group_by_cols,  \n",
    "                 list_numbers_static,\n",
    "                 list_numbers_dinamic, \n",
    "                 list_categories,\n",
    "                 key_join,\n",
    "                 grouper_time = \"M\"):\n",
    "        \"\"\" \n",
    "\n",
    "        :param group_by_cols: Lista de columnas a usar para agrupar los datos.\n",
    "        :type level: list()\n",
    "\n",
    "        :param key_join: Llave para unir data Frames.\n",
    "        :type key_join: Datetime\n",
    "        \n",
    "        :return: New dataframe aggregated\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.group_by_cols = group_by_cols\n",
    "        self.grouper_time = grouper_time\n",
    "        self.key_join = key_join\n",
    "        \n",
    "        self.fill_number_dynamic = list_numbers_dinamic\n",
    "        self.fill_number_static = list_numbers_static\n",
    "        \n",
    "        self.fill_categories = list_categories\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        lst = []\n",
    "        \n",
    "        ## Crear tabla Maestra de llaves\n",
    "        dfmaster = X.groupby(self.group_by_cols)[\"Cantidad_sum\"].sum()\\\n",
    "                            .to_frame()\\\n",
    "                            .reset_index()\\\n",
    "                            .drop_duplicates(subset = self.group_by_cols)[self.group_by_cols]\n",
    "        \n",
    "        ### Recorrer cada combinación\n",
    "        for index, row in tqdm(dfmaster.iterrows()):\n",
    "            conditions_list = []\n",
    "            \n",
    "            ## Crea lista de combinaciones\n",
    "            for col_index in row.index.values:\n",
    "                cond = X[col_index] == row[col_index]\n",
    "                conditions_list.append(cond)\n",
    "            \n",
    "            ## Filtrar dataset\n",
    "            conditions = np.logical_and.reduce(conditions_list)\n",
    "            id_df = X[conditions]\n",
    "            \n",
    "            ### Crear rango de fechas faltantes segun filtro\n",
    "            fechas_rango = pd.date_range(start= id_df.Fecha.min(), \n",
    "                                         end = id_df.Fecha.max(), \n",
    "                                         freq = self.grouper_time)\n",
    "            \n",
    "            dfcalendar = pd.DataFrame({'Fecha': fechas_rango})\n",
    "            for col_index in row.index.values:\n",
    "                dfcalendar[col_index]  = row[col_index]\n",
    "\n",
    "            ### Merge con tabla calendario\n",
    "            df_outer = merge_df(id_df, dfcalendar, [self.key_join] + self.group_by_cols)\n",
    "            \n",
    "            \n",
    "            ### Fillna \n",
    "            for col_number in self.fill_number_dynamic:\n",
    "                df_outer[col_number] =  df_outer[col_number].fillna(0.0)\n",
    "                \n",
    "            for col_number in self.fill_number_static:\n",
    "                df_outer[col_number] =  df_outer[col_number].fillna(method = \"ffill\")\n",
    "                df_outer[col_number] =  df_outer[col_number].fillna(method = \"bfill\")\n",
    "                \n",
    "            for col_cat in self.fill_categories:\n",
    "                df_outer[col_cat] = df_outer[col_cat].fillna(method = \"ffill\")\n",
    "                df_outer[col_cat] = df_outer[col_cat].fillna(method = \"bfill\")\n",
    "                \n",
    "            lst.append(df_outer)\n",
    "            \n",
    "        df_agregado = pd.concat(lst).sort_values('Fecha', ascending = True)\n",
    "\n",
    "        return df_agregado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setCalendar(df_filter, fechadt_col,  dfcalendar, fini_col, ffin_col):\n",
    "    import sys\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    df_filter[fechadt_col] = pd.to_datetime(df_filter[fechadt_col])\n",
    "    \n",
    "    for contador, i in tqdm(enumerate(dfcalendar.iterrows())):\n",
    "        row = i[1]\n",
    "        fini = row[fini_col]\n",
    "        ffin = row[ffin_col]\n",
    "        semana = row[\"Semana\"]\n",
    "        \n",
    "        cond1 = np.logical_and(df_filter[fechadt_col] >= fini, df_filter[fechadt_col] <= ffin)        \n",
    "        df_filter.loc[cond1, fini_col] = fini\n",
    "        df_filter.loc[cond1, ffin_col] = ffin\n",
    "        df_filter.loc[cond1, \"Semana\"] = semana\n",
    "        \n",
    "    return df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Moda(series):\n",
    "    '''\n",
    "    Agregator Function para precio y moda\n",
    "    '''\n",
    "    series = series.astype(str)\n",
    "    x = np.array(series)\n",
    "    unique, counts = np.unique(x, return_counts=True)\n",
    "    return pd.DataFrame({'values': unique, 'count': counts}).sort_values(['count','values'], ascending = False).iloc[0,0]\n",
    "\n",
    "def CountPV(series):\n",
    "    '''\n",
    "    Agregator Function para precio y moda\n",
    "    '''\n",
    "    #x = np.array(series)\n",
    "    #     unique = np.unique(x, return_counts=False)\n",
    "    return len(series.unique())\n",
    "\n",
    "def merge_df(id_df, data_merged, key_join):\n",
    "    return pd.merge(data_merged, id_df, on=key_join , how=\"outer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    El estimador **DateFeatures** se encarga de realizar el muestreo de un dataset segun el porcentaje que se le asigne.\n",
    "    Por ejemplo::\n",
    "\n",
    "        from sklearn import datasets\n",
    "        df = datasets.load_iris()\n",
    "        \n",
    "        list_patterns = ['Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start',\n",
    "                 'Is_year_end', 'Is_year_start','Day', 'Dayofyear','Month']\n",
    "                 \n",
    "        estimator = DateFeatures(list_patterns)\n",
    "        df_sampled = estimator.fit_transform(df)\n",
    "\n",
    "    El ejemplo retorna el dataset con los features de list_patterns creados.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, col_name, list_features):\n",
    "        \"\"\" \n",
    "        Inicializa el parámetro level\n",
    "\n",
    "        :param list_features: representa la lista de date patterns a extraer.\n",
    "        :type level: Integer\n",
    "\n",
    "\n",
    "        Ejemplo::\n",
    "\n",
    "            estimator = sample(0.6)\n",
    "\n",
    "        \"\"\"\n",
    "        self.list_features = list_features\n",
    "        self.col_name = col_name \n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        for pattern in self.list_features:\n",
    "            X.loc[:, \"{}_{}\".format(self.col_name, pattern)] = X[self.col_name].apply(lambda x: getattr(x, pattern.lower()))\n",
    "            X[\"{}_{}\".format(self.col_name, pattern)] = X[\"{}_{}\".format(self.col_name, pattern)].astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateDataBy(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    El estimador **AggregateDataBy** se encarga de agrupar los datos y sacar calculos de las columnas solicitadas en el diccionario.\n",
    "   \n",
    "\n",
    "        Ejemplo::\n",
    "        \n",
    "        from sklearn import datasets\n",
    "        X = datasets.load_iris()   \n",
    "        dict_features = {'cantidad':['sum', test_select, np.count_nonzero, 'max', 'min','std', 'mean'], #suma y moda\n",
    "                 'precio':[ test_select, 'max', 'min', 'std', 'mean'],\n",
    "                 'venta':'sum'} \n",
    "        list_features = [\"idMaterial\",\"Semana\"]\n",
    "        encoder_aggregator = AggregateDataBy(list_features, dict_features)\n",
    "\n",
    "        X_grouped = encoder_aggregator.fit_transform(X)\n",
    "\n",
    "    El ejemplo retorna el dataset agrupado en base a list_features y con los calculos definidos en el diccionario.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, list_aggregators, dict_features):\n",
    "        \"\"\" \n",
    "        Inicializa el parámetro level\n",
    "\n",
    "        :param list_aggregators: Lista de columnas a usar para agrupar los datos.\n",
    "        :type level: Integer\n",
    "\n",
    "        :param dict_features: diccionario de columnas y sus tranformaciones.\n",
    "        :type level: Integer\n",
    "        :return: New dataframe aggregated\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.list_features = list_aggregators\n",
    "        self.dict_features = dict_features\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        X_grouped = X.groupby(self.list_features).agg(dict_features)\n",
    "\n",
    "        return X_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_values(data, name_col):\n",
    "    '''\n",
    "    Hace un Forward fill y un Backward Fill para llenar los valores de una columna. Funcion asociada a roll_precio.\n",
    "    :param data: data filtrada\n",
    "    :type data: pandas DataFrame\n",
    "    \n",
    "    :param name_col: nombre de la columna a trabajar\n",
    "    :type name_col: string\n",
    "    :return data\n",
    "    '''\n",
    "    for col in name_col:\n",
    "        data[col] = data[col].replace(to_replace = 0.0, value = np.nan)\n",
    "    # data.fillna(method = \"ffill\", inplace = True)\n",
    "    \n",
    "    data[name_col] = data[name_col].fillna(method=\"ffill\")\n",
    "    data[name_col] = data[name_col].fillna(method=\"bfill\")\n",
    "    return data\n",
    "\n",
    "\n",
    "class RollFeatureBy(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    El estimador **RollFeatureBy** se encarga de actualizar el valor de una variable en base a valores pasados o valores en el futuro.\n",
    "   \n",
    "        Ejemplo::\n",
    "        \n",
    "        from sklearn import datasets\n",
    "        X = datasets.dataventas()   \n",
    "\n",
    "        list_features_to_roll = [\"PrecioDealer\", \"PrecioMax\"]\n",
    "        encoder_aggregator = RollFeatureBy(list_features_to_roll)\n",
    "\n",
    "        X_rolled = encoder_aggregator.fit_transform(X)\n",
    "\n",
    "    El ejemplo retorna el dataset con los datos actualizados, el precio de un periodo anterior o algún periodo en el futuro.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, list_aggregators, col_name):\n",
    "        \"\"\" \n",
    "        Inicializa el list_features_to_roll\n",
    "\n",
    "        :param list_features_to_roll: Lista de nombre de los features a realizar el roll.\n",
    "        :type level: List      \n",
    "\n",
    "        \"\"\"\n",
    "        self.list_features = list_aggregators\n",
    "        self.col_sku =  col_name\n",
    " \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        df_filled = []\n",
    "\n",
    "        for item in X[self.col_sku].unique():\n",
    "            df_filter = X[X[self.col_sku]== item]\n",
    "            df_filter = df_filter.sort_values('Fecha', ascending = True)\n",
    "            df_filter_processed = fill_values(df_filter, self.list_features)\n",
    "            df_filled.append(df_filter_processed)\n",
    "            \n",
    "        return pd.concat(df_filled).sort_values('Fecha', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Mape\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing Mape\")\n",
    "def mape(y_true,y_pred):\n",
    "    total = len(y_true)\n",
    "    \n",
    "    diferencia = (y_true - y_pred)\n",
    "    \n",
    "    percentage = np.abs(diferencia) / (y_true)\n",
    "    \n",
    "    sumpercenatge = np.sum(percentage)\n",
    "    \n",
    "    result = sumpercenatge/total\n",
    "    return  result*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Improting human_format\")\n",
    "def human_format(num):\n",
    "    num = float('{:.3g}'.format(num))\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '{}{}'.format('{:f}'.format(num).rstrip('0').rstrip('.'), ['', 'K', 'M', 'B', 'T'][magnitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics_regression(actual, pred):\n",
    "    try:\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    except:\n",
    "        rmse = np.inf\n",
    "        \n",
    "    try:    \n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "    except:\n",
    "        mae = np.inf \n",
    "        \n",
    "    try:    \n",
    "        actual_filtrado = actual[actual>0]\n",
    "        pred_filtrado = pred[actual>0]\n",
    "        mape_ = 100-mape((actual_filtrado), (pred_filtrado))\n",
    "        \n",
    "    except:\n",
    "        mape_ = np.inf\n",
    "        \n",
    "    try:    \n",
    "        r2 = r2_score(actual, pred)\n",
    "    except:\n",
    "        r2 = 0.0\n",
    "        \n",
    "#     try:\n",
    "    rmsle = rmsle_score(actual, pred)\n",
    "#     except:\n",
    "#         rmsle = 0.0\n",
    "        \n",
    "    return rmse, mae, r2, mape_, rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics_classification(actual, pred):\n",
    "    try:\n",
    "        report = classification_report(actual, pred)\n",
    "    except:\n",
    "        report = \"no report\"\n",
    "   \n",
    "    try:\n",
    "        roc = roc_auc_score(actual, pred)\n",
    "    except:\n",
    "        roc = \"no roc_auc_score\"\n",
    "        \n",
    "    try:\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "    except:\n",
    "        accuracy = \"no accuracy_score\"\n",
    "        \n",
    "    try:\n",
    "        precision = precision_score(actual, pred)\n",
    "    except:\n",
    "        precision = \"no precision_score\"  \n",
    "\n",
    "    try:\n",
    "        f1 = f1_score(actual, pred)\n",
    "    except:\n",
    "        f1 = \"no f1_score\"  \n",
    "#     try:\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "#     except:\n",
    "        \n",
    "#         cm = \"Couldn't calculate  confusion_matrix\" \n",
    "        \n",
    "    return report, roc, accuracy, precision, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TARGET ENCODE (TIME SERIES)\n",
    "def cat_target(var_cat, target, umbral, param_train, param_test, estimate_train = True):\n",
    "    gr = var_cat + [target]\n",
    "    aux = param_train[gr].groupby(var_cat).agg({target:['mean','size']})\n",
    "    aux.columns = [\"_agg_\".join(x) for x in aux.columns.ravel()]\n",
    "    aux.reset_index(inplace=True)\n",
    "    aux.rename(columns={aux.columns[len(var_cat)]: '-'.join(var_cat)+'_mean_target', aux.columns[len(var_cat)+1]: \"count\"},inplace=True)\n",
    "    aux = aux.loc[aux['count']>=umbral,]\n",
    "    aux.drop(columns = ['count'], inplace=True)\n",
    "    \n",
    "    param_test=param_test.merge(aux, on=var_cat, how=\"left\")\n",
    "    \n",
    "    if estimate_train==True:\n",
    "        param_train=param_train.merge(aux,on=var_cat,how=\"left\")\n",
    "        return param_train, param_test\n",
    "    else:\n",
    "        return param_test\n",
    "\n",
    "def estimate_cross(train,test,var_cat,target,umbral):\n",
    "    n=len(train)\n",
    "    a,b = cat_target(var_cat,target,umbral,param_train=train.loc[0:(n/4),],param_test=train.loc[(n/4)+1:(n/2),],estimate_train=True)\n",
    "    c = cat_target(var_cat,target,param_train=train.loc[0:(n/2),],param_test=train.loc[(n/2)+1:(3*n/4),],estimate_train=0)\n",
    "    d=cat_target(var_cat,target,param_train=train.loc[0:(3*n/4),],param_test=train.loc[(3*n/4)+1:n,],estimate_train=0)\n",
    "    test=cat_target(var_cat,target,param_train=train,param_test=test,estimate_train=0)\n",
    "    \n",
    "    train_new=pd.concat([a,b,c,d],ignore_index=True)\n",
    "    test_new=test\n",
    "\n",
    "    return train_new,test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descripcion_selected_sku(dataset_original, # Dataset del canat con la fecha como index\n",
    "                             canal_name, # Nombre del canal en string\n",
    "                             periodo, # Año de estudio\n",
    "                             selected_sku, # Set o List de skus selectos\n",
    "                             jerarquia='Importe Total (S/.)'): # Feat para ordenar los skus\n",
    "    print('Descripcion SKUs Anualizada: ' + canal_name)\n",
    "    print('Periodo anual: ' + str(periodo)) \n",
    "    dataset = dataset_original.copy()\n",
    "#     dataset['Anio'] = dataset.reset_index().Fecha.apply(lambda x: x.year)\n",
    "#     fechas_df = pd.DataFrame(dataset[dataset.Descripcion.isin(selected_sku)]).groupby('Descripcion')\n",
    "    anual_df = pd.DataFrame(dataset.groupby([pd.Grouper(freq='Y')]+['Descripcion']).agg({\n",
    "                                                                                      'Op':['nunique'],\n",
    "                                                                                      'RUC':['nunique'],\n",
    "                                                                                      'Cantidad':[sum],\n",
    "                                                                                      'Importe':[sum],\n",
    "                                                                                     }))\n",
    "    anual_df.columns = [\"_\".join(x) for x in anual_df.columns.ravel()]\n",
    "    anual_df = anual_df.reset_index()\n",
    "    anual_df['Anio'] = anual_df['Fecha'].apply(lambda x: x.year)\n",
    "    anual_df = anual_df[anual_df.Anio==periodo]\n",
    "    anual_df.drop(columns=['Fecha','Anio'], axis=1 ,inplace=True)\n",
    "    skus = anual_df.Descripcion.nunique()\n",
    "    print('Total SKUs del periodo: {}'.format(skus))\n",
    "    print('Total SKUs seleccionados: {}'.format(len(selected_sku)))\n",
    "    importe = round(anual_df.Importe_sum.sum(),1)\n",
    "    print('Importe total del periodo: S/ {:,.1f}'.format(importe))    \n",
    "    anual_df = anual_df[anual_df.Descripcion.isin(selected_sku)]    \n",
    "    dictio = {\n",
    "              'Descripcion':'Nombre SKU',\n",
    "              'Op_nunique':'Nro. Pedidos',\n",
    "              'RUC_nunique':'Clientes Unicos',\n",
    "              'Cantidad_sum':'Cantidad de SKU',\n",
    "              'Importe_sum':'Importe Total (S/.)'\n",
    "             }\n",
    "    anual_df.rename(columns=dictio, inplace=True)\n",
    "    anual_df = anual_df.sort_values(by=jerarquia, ascending=False)\n",
    "    print('Importe total de los SKUs seleccionados: {:,.1f}'.format(anual_df['Importe Total (S/.)'].sum()))\n",
    "    print('Importe porcentual de los SKUs seleccionados: % {:.2f}'.format(100*anual_df['Importe Total (S/.)'].sum()/importe))\n",
    "    anual_df['Cantidad de SKU'] = anual_df['Cantidad de SKU'].apply(lambda x: '{:.2f} M'.format(x/1000000))\n",
    "    anual_df['Importe Porcentual'] = anual_df['Importe Total (S/.)'].apply(lambda x: '% '+\"%.1f\" % round(100 * x / importe , 1))\n",
    "    anual_df['Importe Total (S/.)'] = anual_df['Importe Total (S/.)'].apply(lambda x: \"%.2f M\" %(x/1000000))\n",
    "    anual_df['Nro. Pedidos'] = anual_df['Nro. Pedidos'].apply(lambda x: '{:,.0f}'.format(x))\n",
    "    return anual_df.set_index('Nombre SKU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Importing PrecioSplitter\")\n",
    "class PrecioSplitter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    El estimador **PrecioSplitter**.\n",
    "    \n",
    "    Por ejemplo::\n",
    "        El ejemplo retorna la parte entera y decimal de una variable continua.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name_col,  int_name =\"IntPrecio\", dec_name = \"FraccionPrecio\",):\n",
    "        \"\"\" \n",
    "        Inicializa el parámetro level\n",
    "\n",
    "        :param name_target: Nombre de la variable target.\n",
    "        :type level: String\n",
    "\n",
    "        Ejemplo::\n",
    "            estimator = PrecioSplitter(name_col = \"PrecioDealer\")\n",
    "            data_precio = estimator.fit_transform(data)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.name_target = name_col \n",
    "        self.int_name = int_name \n",
    "        self.dec_name = dec_name \n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X[(self.int_name)] =  list(map(int, X[self.name_target] //1))\n",
    "        X[(self.dec_name)] = X[self.name_target] % 1\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing ClippedTarget\")\n",
    "class ClippedTarget(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    El estimador **ClippedTarget**.\n",
    "    \n",
    "    Por ejemplo::\n",
    "\n",
    "    El ejemplo retorna el dataset clipeado en base a una medida de dispersión(median, mean, percentil).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name_target, id_grouper, type_clipped, \n",
    "                 clip_low_values=False, \n",
    "                 time_filter = False, \n",
    "                 date_filter  = '2019-12-01'):\n",
    "        \"\"\" \n",
    "        Inicializa el parámetro level\n",
    "\n",
    "        :param name_target: Nombre de la variable target.\n",
    "        :type level: String\n",
    "        :param id_grouper: Nombre de la variable por la que se realizara la agregación.\n",
    "        :type level: String\n",
    "\n",
    "        Ejemplo::\n",
    "        \n",
    "            estimator = ClippedTarget(type_clipped = \"median\")\n",
    "            dataclipped = estimator.fit_transform(data)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.name_target = name_target \n",
    "        self.id_grouper = id_grouper\n",
    "        self.type_clipped = type_clipped\n",
    "        self.time_filter = time_filter\n",
    "        self.clip_low_values = clip_low_values\n",
    "        self.date_filter = date_filter\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        cond_fecha = X[\"Fecha\"] <= self.date_filter\n",
    "        if self.time_filter:\n",
    "            condition = np.logical_and(X[self.name_target]>0, cond_fecha)\n",
    "            dataclipped = pd.DataFrame(X[condition].groupby(self.id_grouper)[self.name_target].agg([self.type_clipped, 'mad'])).reset_index(drop = False)\n",
    "        else:  \n",
    "#             condition = \n",
    "            dataclipped = pd.DataFrame(X[X[self.name_target]>0].groupby(self.id_grouper)[self.name_target].agg([self.type_clipped, 'mad'])).reset_index(drop = False)\n",
    "        \n",
    "        dftotalclipped = pd.merge(X, dataclipped, on = self.id_grouper)\n",
    "        \n",
    "        cond_fecha = dftotalclipped[\"Fecha\"] <= self.date_filter\n",
    "        dftotalclipped[\"maxcantidad\"] = dftotalclipped[self.type_clipped] + 2*dftotalclipped[\"mad\"]\n",
    "        \n",
    "        if self.clip_low_values :\n",
    "            dftotalclipped[\"mincantidad\"] = dftotalclipped[self.type_clipped] - 1.5*dftotalclipped[\"mad\"]\n",
    "            \n",
    "            if self.time_filter:\n",
    "                dftotalclipped.loc[cond_fecha, self.name_target] = dftotalclipped[cond_fecha].apply(lambda x: x[\"mincantidad\"] if x[self.name_target]< x[\"mincantidad\"] and x[self.name_target]>0 else x[self.name_target], axis = 1)\n",
    "                dftotalclipped.loc[cond_fecha, self.name_target] = dftotalclipped.loc[cond_fecha,:].apply(lambda x: x[\"maxcantidad\"] if x[self.name_target]>= x[\"maxcantidad\"] else x[self.name_target], axis = 1)\n",
    "            else: \n",
    "                dftotalclipped.loc[:, self.name_target] = dftotalclipped.loc[:,:].apply(lambda x: x[\"mincantidad\"] if x[self.name_target]< x[\"mincantidad\"] and x[self.name_target]>0 else x[self.name_target], axis = 1)\n",
    "    \n",
    "                dftotalclipped.loc[:, self.name_target] = dftotalclipped.loc[:,:].apply(lambda x: x[\"maxcantidad\"] if x[self.name_target]>= x[\"maxcantidad\"] else x[self.name_target], axis = 1)\n",
    "        \n",
    "        return dftotalclipped, dataclipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMapeCV(dftest_, name_col):\n",
    "    list_df = []\n",
    "\n",
    "    for value_col in dftest[name_col].unique():\n",
    "        dfclient = dftest_[dftest_[name_col] == value_col].copy()\n",
    "        \n",
    "        for sku in dfclient.Descripcion.unique():\n",
    "            dfaux1 = dfclient[dfclient.Descripcion == sku].copy()\n",
    "            y_test = dfaux1[\"Cantidad_sum\"]\n",
    "            y_pred = dfaux1[\"CantidadPred\"]\n",
    "\n",
    "            dftmp = pd.DataFrame({\n",
    "                                'Descripcion':[sku], \n",
    "                                'Kardex':[dfaux1.Kardex.unique()[0]],\n",
    "                                'InversaMape>0': [100-mape(y_test[y_test>0], y_pred[y_test>0])],\n",
    "                                'InversaMape': [100-mape(y_test, y_pred)],\n",
    "                                 name_col: value_col,\n",
    "                                'REAL':dfaux1[\"Cantidad_sum\"].sum(),\n",
    "                                'PRED':dfaux1[\"CantidadPred\"].sum()})\n",
    "            list_df.append(dftmp)\n",
    "\n",
    "    tmp = pd.concat(list_df)    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_to_LSTM_ready(X_unpr, window_size = 5):\n",
    "    l = []\n",
    "    for ind in range(0,int(X_unpr.shape[0])- window_size + 1):\n",
    "        l.append(X_unpr[ind:ind + window_size])\n",
    "    X = np.array(l)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPredLSTM(X_df, y_df, df_train,  batch_size, type_valid = 'Test', include_zeros = False, path = ''):\n",
    "    json_file = open(path+'model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(path+\"model.h5\")\n",
    "\n",
    "    max_test = len(X_df)//batch_size*batch_size\n",
    "    \n",
    "    y_predicted = np.expm1(loaded_model.predict(X_df, batch_size = batch_size))\n",
    "    y_predicted = y_predicted.reshape(len(y_predicted),)\n",
    "    \n",
    "    y_df = np.expm1(list(y_df.reshape(len(y_df),)))\n",
    "    comparison = pd.DataFrame({'predictions' : y_predicted,'actual': y_df})\n",
    "   \n",
    "    comparison['IdMaterial'] = df_to_train[df_to_train.Train_Test == type_valid].IdMaterial.values[:max_test]\n",
    "    comparison['Mes'] = df_to_train[df_to_train.Train_Test == type_valid].Mes.values[:max_test]\n",
    "    comparison['Semana'] = df_to_train[df_to_train.Train_Test == type_valid].Semana.values[:max_test]\n",
    "    comparison['Real'] = np.expm1(df_to_train[df_to_train.Train_Test == type_valid].CantidadVendida.values[:max_test])    \n",
    "    df_tmp = comparison.groupby(['Mes','IdMaterial']).agg({'predictions':'sum','actual':'sum'}).reset_index()\n",
    "    \n",
    "    return df_tmp, comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
